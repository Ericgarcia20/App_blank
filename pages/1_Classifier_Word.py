import streamlit as st
import pandas as pd
import numpy as np
import re
import io
import warnings
warnings.filterwarnings('ignore')

# Try to import scipy, but provide fallback if not available
try:
    from scipy import stats
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    st.warning("‚ö†Ô∏è SciPy not available. Statistical tests will be skipped.")

# Set page configuration
st.set_page_config(
    page_title="Classifier Word Metrics",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better styling
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 5px solid #1f77b4;
    }
    .success-message {
        background-color: #d4edda;
        color: #155724;
        padding: 0.75rem;
        border-radius: 0.25rem;
        border: 1px solid #c3e6cb;
    }
</style>
""", unsafe_allow_html=True)

# Main title
st.markdown('<h1 class="main-header">üìä Classifier Word Metrics</h1>', unsafe_allow_html=True)

# Initialize session state
if 'analysis_complete' not in st.session_state:
    st.session_state.analysis_complete = False
if 'final_df' not in st.session_state:
    st.session_state.final_df = None

# Sidebar configuration
st.sidebar.header("üîß Configuration")

# Default keywords
DEFAULT_KEYWORDS = [
    "personalized", "custom", "tailored", "bespoke", "individual",
    "human", "experience", "connection", "care", "thoughtful", 
    "responsive", "concierge", "attentive", "dedicated", "unique"
]

# Keyword customization
st.sidebar.subheader("üéØ Keyword Dictionary")
keyword_input_method = st.sidebar.radio(
    "Choose input method:",
    ["Use default keywords", "Upload keyword file", "Manual input"]
)

keywords = DEFAULT_KEYWORDS.copy()

if keyword_input_method == "Use default keywords":
    st.sidebar.write(f"Using {len(DEFAULT_KEYWORDS)} default keywords")
    with st.sidebar.expander("View default keywords"):
        st.write(", ".join(DEFAULT_KEYWORDS))

elif keyword_input_method == "Upload keyword file":
    keyword_file = st.sidebar.file_uploader(
        "Upload keyword file (one keyword per line)", 
        type=['txt', 'csv']
    )
    if keyword_file:
        try:
            if keyword_file.name.endswith('.txt'):
                keywords = keyword_file.read().decode('utf-8').strip().split('\n')
            else:
                df_keywords = pd.read_csv(keyword_file)
                keywords = df_keywords.iloc[:, 0].tolist()
            keywords = [k.strip().lower() for k in keywords if k.strip()]
            st.sidebar.success(f"Loaded {len(keywords)} keywords")
        except Exception as e:
            st.sidebar.error(f"Error loading keywords: {str(e)}")
            keywords = DEFAULT_KEYWORDS

elif keyword_input_method == "Manual input":
    keyword_text = st.sidebar.text_area(
        "Enter keywords (one per line):",
        value="\n".join(DEFAULT_KEYWORDS),
        height=200
    )
    if keyword_text:
        keywords = [k.strip().lower() for k in keyword_text.split('\n') if k.strip()]

# Analysis options
st.sidebar.subheader("‚öôÔ∏è Analysis Options")
exact_match = st.sidebar.checkbox("Use exact word matching (recommended)", value=True)
min_word_count = st.sidebar.number_input("Minimum words per post", min_value=1, value=5)

# Helper functions
@st.cache_data
def count_keyword_matches(caption, keywords, exact_match=True):
    """Count keyword matches in caption with improved accuracy"""
    if pd.isna(caption):
        return pd.Series([0, 0, []])
    
    text = str(caption).lower()
    
    if exact_match:
        words = re.findall(r'\b\w+\b', text)
        matched_keywords = [word for word in words if word in keywords]
    else:
        words = re.findall(r'\b\w+\b', text)
        matched_keywords = [word for word in words if any(k in word for k in keywords)]
    
    matched_count = len(matched_keywords)
    total_words = len(words)
    
    return pd.Series([total_words, matched_count, matched_keywords])

def create_visualizations(final_df, likes_corr, comments_corr):
    """Create analysis visualizations using Streamlit's built-in charts"""
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader('Distribution of Personalized Language %')
        # Create histogram data
        hist_data = pd.cut(final_df['match_pct'], bins=20).value_counts().sort_index()
        hist_df = pd.DataFrame({
            'Range': [f"{interval.left:.3f}-{interval.right:.3f}" for interval in hist_data.index],
            'Count': hist_data.values
        })
        st.bar_chart(hist_df.set_index('Range'))
        
        st.subheader(f'Match % vs Comments (r={comments_corr:.3f})')
        chart_data = pd.DataFrame({
            'Match Percentage': final_df['match_pct'],
            'Comments': final_df['number_comments']
        })
        st.scatter_chart(chart_data, x='Match Percentage', y='Comments')
    
    with col2:
        st.subheader(f'Match % vs Likes (r={likes_corr:.3f})')
        chart_data = pd.DataFrame({
            'Match Percentage': final_df['match_pct'],
            'Likes': final_df['number_likes']
        })
        st.scatter_chart(chart_data, x='Match Percentage', y='Likes')
        
        st.subheader(f'Top {min(10, len(final_df))} Most Personalized Posts')
        top_posts = final_df.nlargest(min(10, len(final_df)), 'match_pct')
        if len(top_posts) > 0:
            top_posts_chart = pd.DataFrame({
                'Post Rank': [f"Post {i+1}" for i in range(len(top_posts))],
                'Match %': top_posts['match_pct'].values
            })
            st.bar_chart(top_posts_chart.set_index('Post Rank'))

def get_recommendation(likes_corr):
    """Get recommendation based on correlation"""
    if likes_corr > 0.3:
        return "Strong positive correlation - increase personalized language!"
    elif likes_corr > 0.1:
        return "Moderate correlation - consider A/B testing personalization strategies."
    else:
        return "Weak correlation - personalization may not drive engagement in this dataset."

# Main app layout
col1, col2 = st.columns([2, 1])

with col1:
    st.header("üìÅ Data Upload")
    
    # File upload for main dataset
    uploaded_file1 = st.file_uploader(
        "Upload Instagram posts data (CSV)", 
        type=['csv'],
        key="main_data",
        help="Upload your 'ig_posts_transformed.csv' file or similar dataset with 'ID' and 'Statement' columns"
    )
    
    # File upload for engagement data
    uploaded_file2 = st.file_uploader(
        "Upload engagement data (CSV) - Optional", 
        type=['csv'],
        key="engagement_data",
        help="Upload your engagement data with 'shortcode', 'number_likes', and 'number_comments' columns"
    )

with col2:
    st.header("üìä Analysis Summary")
    if keywords:
        st.metric("Keywords to analyze", len(keywords))
        st.metric("Match type", "Exact" if exact_match else "Partial")
        st.metric("Min word count", min_word_count)

# Process uploaded data
if uploaded_file1 is not None:
    try:
        # Load main dataset
        df = pd.read_csv(uploaded_file1)
        
        st.success(f"‚úÖ Loaded main dataset: {df.shape[0]} rows, {df.shape[1]} columns")
        
        # Validate required columns
        required_cols = ['ID', 'Statement']
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            st.error(f"‚ùå Missing required columns: {missing_cols}")
            st.info("Required columns: ID, Statement")
            st.stop()
        
        # Show dataset preview
        st.subheader("üìã Dataset Preview")
        st.dataframe(df.head(), use_container_width=True)
        
        # Dataset info
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Total Records", len(df))
        with col2:
            st.metric("Unique Posts", df['ID'].nunique())
        with col3:
            st.metric("Missing Statements", df['Statement'].isna().sum())
        with col4:
            st.metric("Columns", len(df.columns))
        
        # Run analysis button
        if st.button("üöÄ Run Analysis", type="primary"):
            with st.spinner("Analyzing personalized language..."):
                
                # Apply keyword matching
                results = df['Statement'].apply(
                    lambda x: count_keyword_matches(x, keywords, exact_match)
                )
                df[['total_words', 'matched_words', 'matched_keywords']] = results
                
                # Calculate match percentage
                df['match_pct'] = np.where(df['total_words'] >= min_word_count, 
                                          df['matched_words'] / df['total_words'], 
                                          0)
                
                # Filter by minimum word count
                df_filtered = df[df['total_words'] >= min_word_count].copy()
                
                # Aggregate to post level
                df_grouped = df_filtered.groupby('ID').agg({
                    'matched_words': 'sum',
                    'total_words': 'sum',
                    'matched_keywords': lambda x: list(set([item for sublist in x for item in sublist]))
                }).reset_index()
                
                df_grouped['match_pct'] = np.where(df_grouped['total_words'] > 0,
                                                  df_grouped['matched_words'] / df_grouped['total_words'],
                                                  0)
                
                # Merge with engagement data if available
                final_df = df_grouped
                
                if uploaded_file2 is not None:
                    try:
                        df_engage = pd.read_csv(uploaded_file2)
                        
                        # Check for required engagement columns
                        engagement_cols = ['shortcode', 'number_likes', 'number_comments']
                        if all(col in df_engage.columns for col in engagement_cols):
                            df_merged = pd.merge(df_grouped, df_engage, 
                                               left_on='ID', right_on='shortcode', 
                                               how='inner')
                            
                            final_df = df_merged[['ID', 'match_pct', 'matched_words', 'total_words', 
                                                 'number_likes', 'number_comments']].copy()
                            
                            st.success(f"‚úÖ Merged with engagement data: {len(final_df)} posts")
                        else:
                            st.warning("‚ö†Ô∏è Engagement data missing required columns. Proceeding without engagement metrics.")
                    
                    except Exception as e:
                        st.warning(f"‚ö†Ô∏è Could not process engagement data: {str(e)}")
                
                # Store results in session state
                st.session_state.final_df = final_df
                st.session_state.analysis_complete = True
                
                st.success("üéâ Analysis completed!")

    except Exception as e:
        st.error(f"‚ùå Error loading data: {str(e)}")
        st.stop()

# Display results if analysis is complete
if st.session_state.analysis_complete and st.session_state.final_df is not None:
    final_df = st.session_state.final_df
    
    st.header("üìà Analysis Results")
    
    # Key metrics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Posts Analyzed", len(final_df))
    with col2:
        posts_with_matches = (final_df['match_pct'] > 0).sum()
        st.metric("Posts with Keywords", posts_with_matches)
    with col3:
        avg_match_pct = final_df['match_pct'].mean()
        st.metric("Avg Match %", f"{avg_match_pct:.1%}")
    with col4:
        total_keywords = final_df['matched_words'].sum()
        st.metric("Total Keywords Found", int(total_keywords))
    
    # Correlation analysis (if engagement data is available)
    if 'number_likes' in final_df.columns and 'number_comments' in final_df.columns:
        st.subheader("üîó Correlation Analysis")
        
        likes_corr = final_df['match_pct'].corr(final_df['number_likes'])
        comments_corr = final_df['match_pct'].corr(final_df['number_comments'])
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Correlation: Match% vs Likes", f"{likes_corr:.4f}")
        with col2:
            st.metric("Correlation: Match% vs Comments", f"{comments_corr:.4f}")
        
        # Interpretation
        if likes_corr > 0.3:
            st.success("üöÄ Strong positive correlation with likes! Consider increasing personalized language.")
        elif likes_corr > 0.1:
            st.info("üìä Moderate correlation with likes. Consider A/B testing personalization strategies.")
        else:
            st.warning("üìâ Weak correlation with likes in this dataset.")
        
        # Statistical significance test
        if SCIPY_AVAILABLE:
            personalized_posts = final_df[final_df['match_pct'] > 0]['number_likes']
            non_personalized_posts = final_df[final_df['match_pct'] == 0]['number_likes']
            
            if len(personalized_posts) > 0 and len(non_personalized_posts) > 0:
                t_stat, p_value = stats.ttest_ind(personalized_posts, non_personalized_posts)
                
                st.subheader("üìä Statistical Analysis")
                col1, col2, col3 = st.columns(3)
                with col1:
                    avg_likes_personalized = personalized_posts.mean()
                    st.metric("Avg Likes (Personalized)", f"{avg_likes_personalized:.0f}")
                with col2:
                    avg_likes_non_personalized = non_personalized_posts.mean()
                    st.metric("Avg Likes (Non-personalized)", f"{avg_likes_non_personalized:.0f}")
                with col3:
                    st.metric("T-test p-value", f"{p_value:.4f}")
                
                if p_value < 0.05:
                    st.success("‚úÖ Statistically significant difference in engagement!")
                else:
                    st.info("‚ÑπÔ∏è No statistically significant difference found.")
        else:
            st.info("üìä Statistical significance tests require SciPy package.")
    
    # Visualizations
    st.subheader("üìä Visualizations")
    
    if 'number_likes' in final_df.columns and 'number_comments' in final_df.columns:
        likes_corr = final_df['match_pct'].corr(final_df['number_likes'])
        comments_corr = final_df['match_pct'].corr(final_df['number_comments'])
        
        create_visualizations(final_df, likes_corr, comments_corr)
    else:
        # Simple histogram if no engagement data using Streamlit's built-in chart
        st.subheader("Distribution of Personalized Language %")
        # Create histogram data
        hist_data = pd.cut(final_df['match_pct'], bins=20).value_counts().sort_index()
        hist_df = pd.DataFrame({
            'Range': [f"{interval.left:.3f}-{interval.right:.3f}" for interval in hist_data.index],
            'Count': hist_data.values
        })
        st.bar_chart(hist_df.set_index('Range'))
    
    # Detailed results table
    st.subheader("üìã Detailed Results")
    
    # Show top personalized posts
    if len(final_df) > 0:
        top_personalized = final_df.nlargest(min(10, len(final_df)), 'match_pct')
        st.write("üèÜ Top Most Personalized Posts:")
        st.dataframe(top_personalized, use_container_width=True)
    
    # Export functionality
    st.subheader("üíæ Export Results")
    
    # Prepare export data
    export_df = final_df.copy()
    export_df['analysis_timestamp'] = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
    export_df['keywords_analyzed'] = len(keywords)
    export_df['match_type'] = 'exact' if exact_match else 'partial'
    
    # Convert to CSV for download
    csv_buffer = io.StringIO()
    export_df.to_csv(csv_buffer, index=False)
    csv_data = csv_buffer.getvalue()
    
    col1, col2 = st.columns(2)
    with col1:
        st.download_button(
            label="üì• Download Results (CSV)",
            data=csv_data,
            file_name=f"ig_analysis_results_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )
    
    with col2:
        # Summary report
        if 'number_likes' in final_df.columns:
            likes_corr = final_df['match_pct'].corr(final_df['number_likes'])
            recommendation = get_recommendation(likes_corr)
            
            summary_text = f"""Instagram Personalized Language Analysis Summary
{'='*50}
Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
Keywords Analyzed: {len(keywords)}
Total Posts: {len(final_df)}
Posts with Personalized Language: {(final_df['match_pct'] > 0).sum()}

Key Findings:
‚Ä¢ Average Personalization Rate: {final_df['match_pct'].mean():.1%}
‚Ä¢ Correlation with Likes: {likes_corr:.4f}
‚Ä¢ Correlation with Comments: {final_df['match_pct'].corr(final_df['number_comments']):.4f}

Recommendation:
{recommendation}
"""
        else:
            summary_text = f"""Instagram Personalized Language Analysis Summary
{'='*50}
Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
Keywords Analyzed: {len(keywords)}
Total Posts: {len(final_df)}
Posts with Personalized Language: {(final_df['match_pct'] > 0).sum()}

Key Findings:
‚Ä¢ Average Personalization Rate: {final_df['match_pct'].mean():.1%}

Note: Upload engagement data for correlation analysis.
"""
        
        st.download_button(
            label="üìÑ Download Summary Report",
            data=summary_text,
            file_name=f"analysis_summary_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.txt",
            mime="text/plain"
        )

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666;'>
    <p>üìä Instagram Personalized Language Analyzer</p>
    <p>Upload your data, customize keywords, and analyze the impact of personalized language on engagement.</p>
</div>
""", unsafe_allow_html=True)
